{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428f1329",
   "metadata": {},
   "source": [
    "## The Aim of this project is to perform Sentiment Analysis using Naive Bayes from Scratch as well as using Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "661c3c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import regex as re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter \n",
    "from copy import deepcopy\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "728789cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Files\n",
    "\n",
    "train_data=pd.read_csv('Data2/all-data.csv')\n",
    "# test_data=pd.read_csv('Data1/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3af8f09b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841</th>\n",
       "      <td>negative</td>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>negative</td>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>negative</td>\n",
       "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4846 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment                                               text\n",
       "0      neutral  According to Gran , the company has no plans t...\n",
       "1      neutral  Technopolis plans to develop in stages an area...\n",
       "2     negative  The international electronic industry company ...\n",
       "3     positive  With the new production plant the company woul...\n",
       "4     positive  According to the company 's updated strategy f...\n",
       "...        ...                                                ...\n",
       "4841  negative  LONDON MarketWatch -- Share prices ended lower...\n",
       "4842   neutral  Rinkuskiai 's beer sales fell by 6.5 per cent ...\n",
       "4843  negative  Operating profit fell to EUR 35.4 mn from EUR ...\n",
       "4844  negative  Net sales of the Paper segment decreased to EU...\n",
       "4845  negative  Sales in Finland decreased by 10.5 % in Januar...\n",
       "\n",
       "[4846 rows x 2 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621c7e1",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c1b6c582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training and Testing data have missing values\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f9299468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9b9465fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "train_data.dropna(inplace=True)\n",
    "# test_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a64975b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    " \n",
    "    # Lowercasing\n",
    "    data['text'] =  data['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "\n",
    "    # Remove \\n, \\\\n\n",
    "    data['text'] =  data['text'].apply(lambda x: x.replace('\\\\n',''))\n",
    "    data['text'] = data['text'].apply(lambda x: x.replace('\\n',''))\n",
    "\n",
    "\n",
    "    # # Remove https\n",
    "    data['text'] =  data['text'].apply(lambda i: re.sub(r\"http\\S+\", \"\", i))\n",
    "\n",
    "#     Remove punctuations\n",
    "    data['text'] =  data['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "#     Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
    "\n",
    "    # # Remove digits\n",
    "    pattern='[0-9]'\n",
    "    \n",
    "    data['text'] =  data['text'].apply(lambda i: re.sub(pattern, '', i))\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d896e493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train=preprocessing(train_data)\n",
    "# test=preprocessing(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99a700",
   "metadata": {},
   "source": [
    "### Sentiment Analysis using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f30d4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bag of Words\n",
    "count_vect = CountVectorizer()\n",
    "bow = count_vect.fit_transform(train['text'])\n",
    "bow = np.array(bow.todense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9ff3dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bow\n",
    "y = train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "101d92ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2d783f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = MultinomialNB().fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "00ce17b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.711340206185567\n",
      "F1 score: 0.6540706094636626\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('F1 score:', f1_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a45d90",
   "metadata": {},
   "source": [
    "### Sentiment Analysis from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d4c49",
   "metadata": {},
   "source": [
    "Bag of Words and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7857c3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(vocab, text):\n",
    "    \n",
    "\n",
    "    words = text.split()\n",
    "    b=Counter(words)\n",
    "    \n",
    "    bag_of_words={**vocab, **b}\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4fb2bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_vocabulary(text):\n",
    "    \n",
    "    vocabulary={}\n",
    "    for tweet in text:\n",
    "#         print(tweet[0])\n",
    "        token=tweet.split()\n",
    "        [vocabulary.update({x: 0}) for x in token if x not in vocabulary.keys()]\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "551d5059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count no of words for each label\n",
    "def count_label_words(data,vocab,labels):\n",
    "\n",
    "    classes=data.groupby('sentiment')\n",
    "    class_count_list=[]\n",
    "    for i in labels:\n",
    "        group=classes.get_group(i)\n",
    "        dictionary_count={x:0 for x in vocab.keys()}\n",
    "        for review in group['text']:\n",
    "\n",
    "            words = review.split()\n",
    "            b=Counter(words)\n",
    "            dictionary_count = {\n",
    "                key: dictionary_count.get(key, 0) + dict(b).get(key, 0) for key in dictionary_count\n",
    "            }\n",
    "\n",
    "           \n",
    "\n",
    "        class_count_list.append(dictionary_count)\n",
    "        \n",
    "    return class_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "3b79b55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(dataset):\n",
    "    \n",
    "    X=dataset[['text','sentiment']]\n",
    "    Y=dataset['sentiment']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    return  X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "823d6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=test_train_split(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "45883ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(count_word_in_class,count_class,vocabulary_len):\n",
    "    \n",
    "    a=count_word_in_class+1\n",
    "    b=count_class+vocabulary_len\n",
    "    return math.log(a/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b313947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNaiveBayes(x, label_words,labels,vocabulary):\n",
    "    \n",
    "    N=len(x) \n",
    "    count=[0,0,0]\n",
    "    \n",
    "    k=0\n",
    "    for i in labels: \n",
    "        count[k]=len(x.groupby('sentiment').get_group(i)) # stores count for each label\n",
    "        k+=1\n",
    "    class_likelihood={}\n",
    "    for i in range(0,len(labels)):\n",
    "        class_likelihood[i]=deepcopy(vocabulary)\n",
    "        \n",
    "    class_priors=[0]*len(labels)\n",
    "    \n",
    "    for l in range(0,len(labels)):\n",
    "        \n",
    "        class_priors[l]= math.log(count[l]/N)\n",
    "        for word in vocabulary.keys():\n",
    "            \n",
    "            #             count of word wi in class c +1/count of all words in class +|v|\n",
    "\n",
    "            Probability= laplace_smoothing(label_words[l][word],sum(label_words[l].values()),len(vocabulary))\n",
    "            class_likelihood[l][word]=  Probability\n",
    "            \n",
    "    return class_likelihood,class_priors\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4c60a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vocabulary\n",
    "vocabulary=create_vocabulary(X_train['text'].to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "519ec165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bag of Words\n",
    "bow=[]\n",
    "\n",
    "for text in X_train['text']:\n",
    "    bow.append(bag_of_words(vocabulary,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "53170701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Word count for each label\n",
    "labels=['positive','negative','neutral']\n",
    "label_count=count_label_words(X_train,vocabulary,labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f894221c",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2700e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_likelihood,class_priors=trainNaiveBayes(X_train, label_count,labels,deepcopy(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0666d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebfe7b86",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "83fd9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNaiveBayes(test,class_likelihood,class_priors,labels,vocabulary):\n",
    "    \n",
    "    predictions=[]\n",
    "    \n",
    "    for text in test:\n",
    "        # storing priors for each class\n",
    "        scores=[prior for prior in class_priors]\n",
    "\n",
    "        words = text.split()\n",
    "\n",
    "        for word in words:\n",
    "            if word in vocabulary.keys(): #ignore words that are not in test data\n",
    "\n",
    "                for l in range(0,len(labels)):\n",
    "#                     Since in class_likelihoods, we have probabilities stored for each word for each class\n",
    "                    scores[l]+=class_likelihood[l][word]\n",
    "        max_value = max(scores)\n",
    "        value=scores.index(max_value)+1\n",
    "        if value==1:\n",
    "            predictions.append('positive')\n",
    "        elif value==2:\n",
    "            predictions.append('negative')\n",
    "        else:\n",
    "            predictions.append('neutral')\n",
    "\n",
    "#         predictions.append(scores.index(max_value)+1)\n",
    "        \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3d3df8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=testNaiveBayes(X_test['text'],class_likelihood,class_priors,labels,deepcopy(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "3c83c069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7010309278350515\n",
      "F1 score: 0.6280812669577336\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, predictions))\n",
    "print('F1 score:', f1_score(y_test, predictions, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d269d9",
   "metadata": {},
   "source": [
    "Dataset used: https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf2b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac7a1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
